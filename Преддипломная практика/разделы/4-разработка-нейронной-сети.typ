#import "@local/gost732-2017:0.4.2": *
#import "@local/bmstu:0.3.0": *
#set math.equation(numbering: "(1)")

= Разработка первой версии
== Выбор архитектуры
Для задачи автоматического отбора фотографий требовалось решение, которое одновременно устойчиво выделяет технические дефекты на разных сценах, достаточно лёгкое для интеграции в приложение и выдаёт раздельные оценки по каждому дефекту, чтобы затем можно было настраивать пороги и логику отбора изображений.

С учётом этого была выбрана архитектура по схеме "общая основа + несколько выходов". Общая часть (основа, бэкбон) извлекает универсальные признаки изображения, а далее идут отдельные выходные блоки (головы, логиты), каждый из которых отвечает за свой дефект. Такой подход особенно удобен в задаче, где дефекты могут встречаться независимо друг от друга: фотография может быть одновременно нерезкой и недоэкспонированной, либо иметь только один дефект. Поэтому вместо выбора одной оценки "хорошо"/"плохо" используется многометочная постановка: сеть выдаёт три независимые оценки - по нерезкости, недосвету и пересвету.

На практике входное изображение подаётся в основу EfficientNetB0 (предобученную на ImageNet, без верхней классификационной части), далее применяется глобальное агрегирование признаков и регуляризация (dropout). Затем от общего вектора признаков отходят три параллельных выхода, каждый из которых предсказывает один логит (одно число) - отдельно для нерезкости, недосвета и пересвета. Таким образом, начальная версия нейронной сети представляла собой EfficientNetB0 + 3 выхода по одному нейрону. На выходе сеть формирует три значения, каждое из которых отражает наличие или отсутствие соответствующего дефекта.

Архитектура схематично представлена на рисунке @общая-архитектура.

#рис(image("../материалы/общая-архитектура.png", width: 80%))[ Архитектура нейронной сети ] <общая-архитектура>

Выбор EfficientNetB0 как основы был сделан на базе результатов научно-исследовательской работы, выполненной в 7 семестре. В рамках НИРа сравнивались несколько популярных архитектур по трём критериям: число параметров, вычислительная сложность и итоговое качество. Рассматривались, в частности, ResNet18, MobileNetV2, EfficientNet-B0 и более тяжёлая архитектура на трансформерах (TinyViT-21M). Сравнение показало, что тяжёлые модели действительно могут давать более высокую точность, но требуют существенно больше вычислений и ресурсов. При этом EfficientNet-B0 продемонстрировала лучший баланс: по качеству она заметно превосходила более простые варианты при умеренном размере и вычислительной нагрузке. Для задачи преддипломной практики этот компромисс оказался оптимальным: модель остаётся достаточно компактной для обучения, экспериментов и встраивания в десктопное приложение, но при этом обеспечивает хороший уровень точности и устойчивости.

// Архитектура EfficientNetB0 представлена на рисунке @архитектура-B0.

// #рис(image("../материалы/B0-архитектура.png", width: 100%))[ Архитектура EFficientNetB0 ] <архитектура-B0>

\
\
\
\
\

== Подготовка датасета
=== Выбор источника фотографий
Для обучения нейронной сети, определяющей технические дефекты фотографий, требуется набор данных с большим количеством изображений и корректной разметкой по целевым дефектам. При этом важно, чтобы разметка отражала именно техническое качество (нерезкость, недосвет, пересвет), а не смысл или «удачность» сцены. То есть разметка должна быть результатом не субъективной оценки "нравится"/"не нравится" опрошенных людей. На практике получить такой набор данных непросто: ручная разметка тысяч фотографий требует значительных трудозатрат и остаётся частично субъективной, особенно в пограничных случаях.

При формировании данных возможны два подхода. Первый - использование реальных датасетов с оценками качества. Такие наборы ближе к реальным пользовательским условиям, однако часто содержат только общую оценку качества без явного указания типа дефекта. Кроме того, часть датасетов ориентирована на эстетическую (т.е. субъктивную) оценку, что усложняет выделение конкретных технических причин брака. 

Второй подход - формирование синтетического датасета на основе исходных качественных изображений, когда к «чистым» фотографиям программно добавляются контролируемые искажения (размытие, изменение яркости). В этом случае разметка формируется автоматически и не зависит от субъективной оценки, можно получить достаточное число примеров каждого дефекта, управлять силой дефекта через уровни искажений и обеспечивать воспроизводимость экспериментов.

С учётом ограничений преддипломной практики (необходимость быстро получить достаточный объём данных с однозначной разметкой) более рациональным является второй подход - обучение на синтетических искажениях.

\
\
\

В качестве источника данных выбран датасет KADIS-700K. Он содержит исходные изображения и большое число их версий с искусственно добавленными искажениями различных типов и уровней. Это позволяет сформировать выборку, напрямую соответствующую поставленной задаче: выделить только те типы искажений, которые моделируют нерезкость (размытие/смаз), недосвет и пересвет, и автоматически сформировать разметку для обучения нейронной сети.

KADIS-700K- крупный синтетический набор для оценки качества изображений. Он включает около 140000 исходных (неискажённых) изображений и примерно 700000 искажённых: для каждого исходного изображения подготовлено несколько вариантов, полученных добавлением искажений, выбираемых из набора 25 типов с 5 уровнями выраженности. Набор искажений формируется с помощью кода генерации искажений в MATLAB, что позволяет точно знать тип и уровень добавленного дефекта.

В рамках дипломного проекта рассматриваются только три дефекта, и поэтому в обучении не будут участвовать все 840000 изображений, но на их базе можно сформировать достаточную выборку изображений с нужными дефектами. Из всего спектра искажений KADIS-700K используются только следующие типы:

1) нерезкость/смаз: lens blur (dist_type = 2) и motion blur (dist_type = 3);

2) пересвет: brighten (dist_type = 16);

3) недосвет: darken (dist_type = 17).


\
\
\
\
\
\
\
\
\

=== Генерация искаженных изображений
Для формирования обучающей выборки на основе KADIS-700K использовался исходный скрипт генерации, поставляемый вместе с кодом добавления искажений (code_imdistort) и функцией imdist_generator. В оригинальном варианте скрипт последовательно проходил по списку исходных изображений из файла kadis700k_ref_imgs.csv, считывал каждое изображение из каталога ref_imgs/ и для заданного типа искажения генерировал все пять уровней (от 1 до 5). Исходный код добавления искажений представлен в листинге @матлаб-исходник.

#листинг(```
%% setup
clear; clc;
addpath(genpath('code_imdistort'));


%% read the info of pristine images

tb = readtable('kadis700k_ref_imgs.csv');
tb = table2cell(tb);

%% generate distorted images in dist_imgs folder

for i = 1:size(tb,1)
    ref_im = imread(['ref_imgs/' tb{i,1}]);
    dist_type = tb{i,2};
    
    for dist_level = 1:5
        [dist_im] = imdist_generator(ref_im, dist_type, dist_level);
        strs = split(tb{i,1},'.');
        dist_im_name = [strs{1}  '_' num2str(tb{i,2},'%02d')  '_' num2str(dist_level,'%02d') '.bmp'];
        disp(dist_im_name);
        imwrite(dist_im, ['dist_imgs/' dist_im_name]);
    end
    
end
```)[ Исходный код добавления искажений ] <матлаб-исходник>

В рамках преддипломной практики данный скрипт был адаптирован под задачи дипломного проекта. Во-первых, из полного набора искажений KADIS-700K были оставлены только те, которые соответствуют целевым дефектам. Для этого после чтения таблицы выполнялась фильтрация по dist_type, и дальнейшая генерация выполнялась только для типов 2, 3, 16 и 17. 

Во-вторых, вместо перебора всех уровней искажения для каждого изображения уровни выбирались случайно из заранее заданных допустимых наборов, что позволило сконцентрировать данные на более заметных дефектах и избежать слабых искажений, которые хуже соответствуют практическим ситуациям. Использовались следующие диапазоны: для dist_type = 2 уровни [2, 3, 4], для dist_type = 3 уровни [4, 5], для dist_type = 16 уровни [4, 5], для dist_type = 17 уровни [4, 5].

В-третьих, изменён формат сохранения искажённых изображений. Вместо исходного расширения .bmp результаты сохранялись в .jpg с параметром качества 95. Такой выбор сделан для приближения данных к реальным пользовательским фотографиям (в большинстве случаев они хранятся в JPEG), а также для ускорения чтения с диска и уменьшения занимаемого объёма памяти. Имена файлов формировались по схеме "name_XX_YY.jpg", где к базовому имени исходного файла добавлялись код типа искажения XX и уровень YY, записанные в двухзначном формате.

Дополнительно в скрипт были внесены проверки, повышающие устойчивость генерации. Если целевой файл уже существует в dist_imgs/ (т.е. уже есть его искаженная версия), генерация пропускается, что позволяет безопасно перезапускать процесс без перезаписи уже сформированных данных. К тому же это позволило снизить избыточную корреляцию в данных: если сохранять множество вариантов одной и той же сцены, модель может переобучаться на характерные детали конкретного изображения и демонстрировать завышенные метрики вместо устойчивого распознавания дефектов на разных сценах.

Модифицированный код представлен в листинге @матлаб-модификация и в приложении А.

#листинг(```
%% setup
clear; clc;
addpath(genpath('code_imdistort'));

fid = fopen('labels.csv','w');
fprintf(fid, 'path,blur,under,over,dist_type,dist_level,ref\n');

tb = readtable('kadis700k_ref_imgs.csv');

% --- ОСТАВЛЯЕМ ТОЛЬКО НУЖНЫЕ ИСКАЖЕНИЯ ---
types = [2 3 16 17];            % gaussian/lens/motion blur + brighten + darken
tb = tb(ismember(tb{:,2}, types), :);  % 2-й столбец = dist_type

tb = table2cell(tb);

ref_list = unique(tb(:,1)); 

for r = 1:numel(ref_list)
    ref_name = ref_list{r};
    ref_path = fullfile('ref_imgs', ref_name);

    % дефектов нет
    blur = 0; under = 0; over = 0;
    dist_type = 0; dist_level = 0;

    fprintf(fid, '%s,%d,%d,%d,%d,%d,%s\n', ref_path, blur, under, over, dist_type, dist_level, ref_name);
end


%% generate distorted images in dist_imgs folder

for i = 1:size(tb,1)
    ref_im = imread(fullfile('ref_imgs', tb{i,1}));
    dist_type = tb{i,2};

    if dist_type == 2
        allowed_levels = [2 3 4];
    elseif dist_type == 3
        allowed_levels = [4 5];
    elseif dist_type == 16
        allowed_levels = [4 5];
    elseif dist_type == 17
        allowed_levels = [4 5];
    else
        continue;
    end
    
  %%  for k = 1:numel(keep_levels)
        dist_level = allowed_levels(randi(numel(allowed_levels)));

        dist_im = imdist_generator(ref_im, dist_type, dist_level);

        % имя файла остаётся совместимым: _тип_уровень.bmp
        strs = split(tb{i,1},'.');

        dist_im_name = [strs{1} '_' num2str(dist_type,'%02d') '_' num2str(dist_level,'%02d') '.jpg'];
        out_path = fullfile('dist_imgs', dist_im_name);

        if exist(out_path, 'file')
            continue;
        end
        
        imwrite(dist_im, out_path, 'Quality', 95);

        % вычисляем метки по dist_type
        blur = 0; under = 0; over = 0;
        if dist_type == 2 || dist_type == 3
            blur = 1;
        elseif dist_type == 17
            under = 1;
        elseif dist_type == 16
            over = 1;
        end
       
        fprintf(fid, '%s,%d,%d,%d,%d,%d,%s\n', out_path, blur, under, over, dist_type, dist_level, tb{i,1});

 %%   end
end

fclose(fid);

```)[ Модифицированный код добавления искажений ] <матлаб-модификация>

В результате сгенерированы искажённые изображения, соответсвующие выбранным дефектам дипломного проекта и сохраненные в формате, близком к условиям практического использования в приложении.

\

=== Разделение датасета
После генерации искажённых изображений был сформирован единый файл разметки labels.csv, содержащий пути к изображениям и целевые метки дефектов. Для дальнейшего обучения нейронной сети требовалось разделить данные на обучающую и валидационную выборки так, чтобы оценка качества была корректной и не завышалась из-за попадания в обе выборки разных версий одной и той же сцены.

Разделение "по строкам" в данном случае является нежелательным, так как в датасете присутствуют несколько вариантов одного исходного изображения (исходное и искажённая версии). Если часть вариантов одной сцены окажется в обучающей выборке, а другая часть - в валидационной, нейронная сеть может демонстрировать завышенные метрики за счёт узнавания особенностей конкретной сцены, а не за счёт устойчивого распознавания дефектов. Чтобы избежать такой утечки, разделение выполнялось не по отдельным файлам, а по имени исходного изображения.

В разметке для этого используется поле ref, которое хранит имя исходного файла и тем самым группирует все варианты одной сцены. В скрипте сначала загружается labels.csv, затем выполняется разделение по уникальным значениям ref. Множество исходных сцен случайно разбивается на две части в пропорции 80/20. После этого строки, относящиеся к выбранным ref, собираются в train_df, а остальные - в val_df. Полученные таблицы сохраняются в файлы train.csv и val.csv.

Скрипт, разделяющий датасет на обучающую и валидационную выборки содержится в листинге @скрипт-разметка и в приложении A.

#листинг(```
import os
import pandas as pd
from sklearn.model_selection import train_test_split

LABELS_PATH = "labels.csv"
TRAIN_OUT = "train.csv"
VAL_OUT = "val.csv"
VAL_RATIO = 0.2
SEED = 42

df = pd.read_csv(LABELS_PATH)

df["path"] = df["path"].astype(str).str.replace("\\", os.sep).str.replace("/", os.sep)

# Сплит по ref (чтобы не было утечки)
refs = df["ref"].astype(str).unique()
train_refs, val_refs = train_test_split(refs, test_size=VAL_RATIO, random_state=SEED, shuffle=True)

train_df = df[df["ref"].astype(str).isin(train_refs)].reset_index(drop=True)
val_df   = df[df["ref"].astype(str).isin(val_refs)].reset_index(drop=True)

train_df.to_csv(TRAIN_OUT, index=False)
val_df.to_csv(VAL_OUT, index=False)

print("Saved:", TRAIN_OUT, len(train_df))
print("Saved:", VAL_OUT, len(val_df))
print("Unique refs train/val:", len(train_refs), len(val_refs))
```)[ Скрипт, разделяющий датасет на обучающую и валидационную выборки ] <скрипт-разметка>

В результате такой процедуры обеспечивается отсутствие пересечения сцен между выборками: одна и та же исходная фотография и её искажённая версия могут принадлежать только обучающей или только валидационной части. Это позволяет получить более честную оценку качества нейронной сети и избежать завышения метрик из-за утечки данных. В результате мы получили готовый разделенный датасет, включающий в себя как данные, так и разметку.

== Первая итерация
Обучение организовано через TensorFlow с использованием конвейера tf.data, так как разметка задаётся таблицами train.csv и val.csv, а не структурой папок вида класс/изображения. На этапе подготовки данных путь к файлу считывается из CSV, затем изображение загружается функцией tf.io.read_file и декодируется универсальным декодером tf.image.decode_image, который позволяет работать с форматами JPG/PNG/BMP. После декодирования изображение приводится к типу float32 и нормируется в диапазон [0; 1], а далее масштабируется до фиксированного размера 512x384 пикселей. Такой размер был выбран как компромисс между сохранением деталей дефектов (в первую очередь нерезкости) и ограничениями по ресурсам видеопамяти при обучении. Для обучающей выборки дополнительно применяется перемешивание, затем данные группируются в пакеты (батчи) размером 16 изображений и подаются на обучение с предварительной подгрузкой, что уменьшает простои вычислений из-за ввода-вывода.

Выход основы (EfficientNetB0) представляет собой пространственную карту признаков, которая преобразуется в вектор с помощью глобального усредняющего пуллинга (GlobalAveragePooling2D). Далее применяется Dropout с вероятностью 0.2: этот слой на обучении случайно "отключает" часть нейронов и тем самым снижает риск переобучения. После этого реализованы три независимые головы - по одному нейрону на каждый дефект. Важно, что выходы голов являются логитами: логит - это значение до применения сигмоиды, а вероятность дефекта получается применением функции sigmoid(logit).

В качестве функции потерь используется бинарная кросс-энтропия с параметром from_logits=True. Это означает, что функция потерь сама применяет сигмоиду внутри себя и корректно работает с логитами. Для контроля качества в процессе обучения применяется метрика AUC, причём в многометочном варианте. Бинарная кросс-энтропия рассчитывается по формуле @бинарная-кросс-энтропия.

$ L(y,p) = -[y log(p) + (1-y)log(1-p)], $ <бинарная-кросс-энтропия>
#v(-1.5em)
где $L(y,p)$ - значение функции потерь,\
$y$ - истинная метка ( $y \in {0,1}$ ),\
$p$ - предсказанная моделью вероятность класса 1 ( $0 < p < 1$ ).

Первая итерация проводилась в два этапа. На первом этапе обучались только три выходные головы, а основа EfficientNetB0 была заморожена (backbone.trainable=False). Скорость обучения на этом этапе была выбрана 10^(-3). На втором этапе основа размораживалась полностью (backbone.trainable=True) и выполнялось дообучение всей сети с уменьшенной скоростью обучения 10^(-4). Уменьшение шага здесь принципиально: дообучение предобученной основы требует более аккуратных обновлений, иначе можно быстро получить деградацию признаков и нестабильность обучения. Размер пакета 16 выбран исходя из практического баланса: он достаточно велик для приемлемой стабильности градиентов, но при размере входа 512x384 не приводит к переполнению видеопамяти на видеокарте "домашнего" компьютера.

Динамика обучения показывает, что выбранная схема действительно даёт прирост качества. По итогам первого этапа (5-я эпоха обучения только голов) обучающая AUC достигла 0.6063 при значении функции потерь 0.4276, а на проверочной выборке AUC составила 0.7151 при val_loss = 0.4325. После перехода ко второму этапу и дообучения основы качество на проверке выросло существенно выше. В конце обучения достигнуты значения auc = 0.8048, loss = 0.3496, val_auc = 0.8627, val_loss = 0.3205. Таким образом, именно после дообучения основы модель значительно лучше отделяет дефектные изображения от качественных по всем трём меткам. Конкретные значения loss и auc в зависимости от эпох прежставлены на рисунке @журнал-первое.

#рис(image("../материалы/первое поколение.jpg", width: 80%))[ Значения loss и AUC первой итерации ] <журнал-первое>

На графиках обучения заметна характерная проблема: проверочная функция потерь val_loss ведёт себя нестабильно и даёт резкие всплески на последних 5 эпохах, несмотря на общий рост AUC. Это поведение связано с тем, что при полном размораживании основы начинают активно обновляться слои пакетной нормализации BatchNorm. BatchNorm (пакетная нормализация) использует статистики текущего пакета и поддерживает скользящие оценки среднего и дисперсии, которые затем влияют на поведение модели на проверке и при реальном использовании. При небольшом размере пакета (в данном случае 16) оценки статистик получаются шумными, а их обновление может приводить к тому, что модель подстраивается под особенности обучающих пакетов и временно ухудшается на проверке. Визуально это проявляется скачками val_loss и колебаниями val_auc. В дальнейшем эта проблема была решена заморозкой BatchNorm на этапе дообучения основы. Графики обучения представлены на рисунке @первое-графики.

#рис(image("../материалы/первое_поколение_training_curves.png", width: 70%))[ Графики ключевых метрик первой итерации ] <первое-графики> 

Дополнительно качество было оценено отдельным скриптом на проверочной выборке в терминах точности (т.е. в более понятных человеку процентах угадывания) при фиксированном пороге решения. Получены значения: общая точность по всем меткам составила 0.8333, отдельно по дефектам - 0.7492 для blur, 0.8725 для under и 0.8784 для over. Результаты проверки точности представлены на рисунке @первое-точность.

#рис(image("../материалы/первое_поколение_accuracy.jpg", width: 80%))[ Точность правильных предсказаний первой итерации ] <первое-точность>

\
\

В целом первая итерация подтвердила работоспособность выбранного направления: трёхголовая архитектура на EfficientNetB0 обучается на синтетических искажениях и демонстрирует высокую способность различать дефекты, а двухэтапная схема обучения существенно улучшает качество по сравнению с обучением только выходных слоёв. Одновременно были выявлены практические ограничения, связанные с нестабильностью проверки при дообучении основы из-за BatchNorm, что стало основанием для последующих улучшений во второй итерации модели. Программный код первой итерации представлен в приложении А.

== Вторая итерация
Во второй итерации нейронной сети необходимо доработать первую итерацию так, чтобы убрать нестабильность на проверке, которую давали слои пакетной нормализации; усилить распознавание нерезкости как самой слабой части модели и сделать контроль обучения более прозрачным за счёт расширенного набора метрик и автоматического построения графиков.

В первой итерации каждая голова представляла собой 1 нейрон. По результатам первой проверки именно нерезкость распознавалась хуже остальных: точность по blur на проверке была заметно ниже (около 0.75), тогда как under/over были около 0.87–0.88. Это объяснимо: нерезкость проявляется в тонких деталях (высоких частотах), и для её отделения от требуется более сложная нелинейная комбинация признаков, чем для экспозиционных дефектов, которые сильнее зависят от распределения яркостей.

Поэтому во второй итерации для головы, отвечающей за размытие, добавлена цепочка слоёв 64 -> 32 -> 16 -> 1 с ReLU-активацией и Dropout 0.15. Для under/over усиление более умеренное: 32 -> 16 -> 1 и Dropout 0.1. Такой перекос по мощности сделан осознанно: вычислительная цена растёт незначительно (эти слои стоят после глобального пуллинга и работают с вектором признаков), но модель получает возможность построить более сложную границу решения именно для blur - там, где первая итерация была слабее.

Вторая важная доработка касается устойчивости дообучения основы. В первой итерации при размораживании EfficientNet график функции потерь заметно "скакал", что связано с BatchNorm. Во второй итерации эта проблема решена двумя мерами одновременно. Во-первых, на этапе тонкой настройки BatchNorm-слои принудительно замораживаются (их параметры и статистики перестают обновляться). Во-вторых, размораживается не вся основа, а только её верхняя часть: все слои, кроме последних примерно 40, оставляются замороженными. Тем самым сохраняются универсальные низкоуровневые признаки (контуры, текстуры), а адаптация идёт за счёт верхних слоёв, которые отвечают за более сложные комбинации признаков. Дополнительно уменьшается скорость обучения на втором этапе до 5*10^(-5), чтобы изменения весов были плавными и не приводили к деградации на проверке. В результате проверочные кривые становятся заметно более стабильными: на итоговом графике нет резких провалов val_loss, а к концу обучения значения сходятся к небольшим величинам. Графики обучения представлены на рисунке @второе-графики.

#рис(image("../материалы/второе_поколение_training_curves.png", width: 70%))[ Графики ключевых метрик второй итерации ] <второе-графики> 

Третья группа изменений - расширение набора метрик и улучшение журналирования. В первой итерации была только общая AUC, и по ней сложно понять, какая из голов учится хуже и почему. В конце концов скрипт по точности предсказаний дал понять, что самый слабый выход - выход по резкости, но такая проверка на выходе дает слабое представление о том, как каждая из голов обучалась. Во второй итерации добавлены метрики бинарной кросс-энтропии, посчитанной отдельно по каждому выходу. Новые графики также можно наблюдать на рисунке @второе-графики.

Финальные числа показывают, что доработки дали существенный прирост и по общим метрикам, и по каждой голове. В конце обучения получены значения: AUC на обучении 0.9959, AUC на проверке 0.9822, при этом проверочные потери составили val_loss = 0.0833. Финальные метрики представлены на рисунке @журнал-второе.

#рис(image("../материалы/второе_поколение.jpg", width: 80%))[ Значения loss и AUC второй итерации ] <журнал-второе>

Отдельный скрипт оценки на проверочной выборке показывает прикладные метрики качества при порогах 0.5 для всех голов. Общая точность по всем меткам (micro-accuracy) составила 0.9713, а точность строгого совпадения трёх меток одновременно (exact-match accuracy - доля изображений, где верно угаданы сразу все три метки) составила 0.9179. По дефектам получены следующие показатели. Для blur: precision 0.9946, recall 0.9866, F1 0.9906, accuracy 0.9953 (precision - доля правильных среди всех срабатываний, recall - доля найденных дефектов среди всех реальных дефектов). Для under: precision 0.7709, recall 0.7986, F1 0.7845, accuracy 0.9440 - видно, что именно under даёт основную часть ложных срабатываний (FP=271), поэтому его точность ниже. Для over: precision 0.9241, recall 0.8604, F1 0.8911, accuracy 0.9744, то есть пересвет распознаётся уверенно, но чуть хуже blur. Результаты проверки точности представлены на рисунке @второе-точность.

Приечание: precision - доля истинно-положительных среди всех предсказанных положительных, recall - доля найденных истинно-положительных среди всех реальных положительных, F1 - гармоническое среднее precision и recall, а accuracy - доля верных предсказаний среди всех предсказаний.

#рис(image("../материалы/второе_поколение_accuracy_доп.jpg", width: 55%))[ Точность предсказаний второй итерации ] <второе-точность>

Модель уже достигла высокого уровня разделения классов на проверке (val_auc 0.9822) и высокой практической точности по всем меткам (micro-accuracy 0.9713), а графики показывают выход метрик на плато без признаков систематического улучшения к концу 10-й эпохи. После стабилизации BatchNorm и частичного дообучения основы дальнейшие эпохи с большой вероятностью дадут убывающую отдачу и увеличат риск переобучения под синтетические искажения.

Отдельно стоит обсудить метрику under. У метки under много ложноположительных срабатываний (FP=271), особено в сравнении с другими выходами. Поэтому её точность ниже (precision 0.7709 при accuracy 0.9440). Чаще всего это ночные/темные сцены: кадр намеренно тёмный, но модель воспринимает это как недосвет. Для over эта проблема тоже прослеживается, но не так явно, потому что "естественные" затемнения в реальном мире встречаются намного чаще сильных пересветов. Проблему количества ложноположительных срабатываний головы under не решить увеличением количества параметров или изменением датасета. Решение этой проблемы требует пересмотра архитектуры нейронной сети.

Таким образом, на второй итераци устранена нестабильность проверки, усилена самая слабая голова (blur), добавлен детальный контроль обучения по метрикам, а итоговые числа подказывают, что выход under требует доработки.

Программный код второй итерации представлен в приложении А.

= Разработка второй версии

== Доработка архитектуры

После экспериментов с первой версией модели была проведена доработка архитектуры. Базовая схема "одна общая часть + несколько выходов" была сохранена. Основное изменение связано с добавлением четвертой головы night, предназначенной для распознавания ночных сцен. Это связано с тем, что under может ошибочно срабатывать на ночных кадрах, где низкая яркость является нормой, а не дефектом. Голова night реализована как отдельный выход модели и используется в прикладной логике: при высокой вероятности ночной сцены фильтрацию по under можно ослаблять или отключать, чтобы не браковать корректные ночные фотографии - это должно помочь повысить точность головы under и снизить ее количество ложноположительных срабатываний непосредственно в прикладном приложении. Примечание - здесь и далее понятие "ночная сцена" не обязательно означает в прямом смысле ночную сцену (хотя и в том числе), но так же включает в себя темные и слабоосвещенные фотографии, сделанные, например, в условиях плохого освещения. Итоговая архитектура схематично представлена на рисунке @общая-архитектура-2.

#рис(image("../материалы/общая-архитектура-2.png", width: 80%))[ Архитектура нейронной сети ] <общая-архитектура-2>

От идеи дообучить under распознавать ночные сцены решено отказаться, потому что в прикладном приложении планируется разбивать фотографию большого разрешения на множество частей размером 512x384 пикселей, каждая из которых будет проходить через нейронную сеть и на основе общей оценки всех частей будет выдаваться вердикт о качестве фотографии. 

Распознавание же ночных сцен невозможно по отдельным частям фотографии, потому что такое распознавание требует контекста всей сцены. Например, на одной фотографии темным может быть звездное небо, а светлым - здание на переднем плане. Если пытаться распознать ночь "по частям", то части со звездным небом будут распознаваться как ночные, а части со зданием - как не ночные. В общем случае сложно сказать, какая часть изображения должна быть "ночной", чтобы считать всю фотографию ночной, в отличие от других меток under и over, по которым в сообществе фотографов существует примерное мнение о допустимом объеме недосвета или пересвета.

== Расширение датасета

=== Выбор источника фотографий

Для обучения дополнительной головы night требовался набор изображений, где можно устойчиво отделять ночные сцены от обычных дневных/светлых. В рамках работы было принято решение собрать отдельный датасет из двух источников, чтобы обеспечить достаточное разнообразие сцен и получить явную разметку на уровне ночь/не ночь без ручной пометки каждого кадра.

В качестве источника ночных сцен использовался датасет ExDark. Его содержимое ориентировано на условия низкой освещённости, поэтому все изображения из этой папки рассматривались как примеры класса night = 1. Для этих файлов метки технических дефектов blur и over фиксировались в ноль (т.к. на этом датасете будет обучаться только голова night, головы blur, over и under обучаться не будут), а метка under назначалась случайно (0 или 1), поскольку цель данного источника - именно обучение распознаванию ночной сцены, а не точная оценка недосвета как дефекта.

В качестве источника "обычных" сцен использовался датасет MIT Indoor Scenes, преимущественно состоящий из светлых фотографий. Все изображения из этого источника считались примерами класса night = 0. Чтобы одновременно иметь в этом источнике как примеры без дефекта, так и примеры с дефектом under, внутри каждой категории изображения делились на две равные части: для первой половины задавалась метка under = 1, для второй- under = 0. Метки blur и over также фиксировались в ноль. Такой подход позволяет получить контролируемую смесь "нормальных" и "искусственно недосвеченных" дневных сцен. 

=== Генерация искаженных изображений

Искажения для изображений, помеченных как under = 1 и night = 0, создавались автоматически в MATLAB. Для таких файлов применялся тип искажения 17 (darken) с уровнями 4–5, после чего изображение перезаписывалось без изменения имени. Логика генерации затемнения и вызов функции искажения были реализованы на основе ранее использованных MATLAB-скриптов генерации искажений для KADIS-700K: была переиспользована проверенная функция imdist_generator и общий принцип "тип искажения + уровень -> получение изменённого изображения". Одновременно в night_labels.csv обновлялись поля dist_type и dist_level, чтобы разметка соответствовала фактически применённому преобразованию. Модифицированный код представлен в листинге @матлаб-модификация-2 и в приложении А.

#листинг(```
%% apply_under_to_day_images_from_night_labels
% Читает night_labels.csv (path,blur,under,over,night,dist_type,dist_level,ref)
% Если under==1 и night==0 -> применяет искажение dist_type=17 (darken)
% и ПЕРЕЗАПИСЫВАЕТ изображение ПО ТОМУ ЖЕ ПУТИ (имя не меняется).
%
% ВАЖНО:
% 1) Этот скрипт изменяет исходные файлы. Для подстраховки можно включить BACKUP.
% 2) dist_level выбирается случайно из [4 5] (как у тебя в KADIS).
% 3) dist_type/dist_level в таблице обновляются и CSV перезаписывается.

%% setup
clear; clc;
addpath(genpath('code_imdistort'));

INPUT_CSV  = 'night_labels.csv';
OUTPUT_CSV = 'night_labels.csv';  % перезаписать этот же файл

% Подстраховка: сделать копию файла перед перезаписью (1 = да, 0 = нет)
BACKUP_CSV = 1;
BACKUP_DIR = 'backup_before_under';

% Для 17 типа (darken) берём уровни как в проекте
allowed_levels_17 = [4 5];

% Качество JPEG при перезаписи (если файл jpg/jpeg)
JPEG_QUALITY = 95;

%% read labels
tb = readtable(INPUT_CSV);

% базовая проверка колонок
needCols = {'path','blur','under','over','night','dist_type','dist_level','ref'};
for c = 1:numel(needCols)
    if ~ismember(needCols{c}, tb.Properties.VariableNames)
        error('В CSV нет обязательной колонки: %s', needCols{c});
    end
end

if BACKUP_CSV
    if ~exist(BACKUP_DIR, 'dir'), mkdir(BACKUP_DIR); end
    copyfile(INPUT_CSV, fullfile(BACKUP_DIR, ['night_labels_backup_' datestr(now,'yyyymmdd_HHMMSS') '.csv']));
end

%% find rows to process: under=1 and night=0
mask = (tb.under == 1) & (tb.night == 0);

idx = find(mask);
fprintf('[INFO] Найдено строк для обработки (under=1 & night=0): %d\n', numel(idx));

if isempty(idx)
    fprintf('[INFO] Нечего делать. Выход.\n');
    return;
end

%% process each image
processed = 0;
skipped_missing = 0;
skipped_readerr  = 0;

for k = 1:numel(idx)
    i = idx(k);

    img_path = tb.path{i};

    if ~isfile(img_path)
        skipped_missing = skipped_missing + 1;
        fprintf('[WARN] Файл не найден: %s\n', img_path);
        continue;
    end

    try
        im = imread(img_path);
    catch
        skipped_readerr = skipped_readerr + 1;
        fprintf('[WARN] Не удалось прочитать: %s\n', img_path);
        continue;
    end

    % выбираем уровень затемнения
    dist_type  = 17;
    dist_level = allowed_levels_17(randi(numel(allowed_levels_17)));

    % применяем искажение
    dist_im = imdist_generator(im, dist_type, dist_level);

    % перезаписываем БЕЗ изменения имени
    [~,~,ext] = fileparts(img_path);
    ext = lower(ext);

    try
        if strcmp(ext, '.jpg') || strcmp(ext, '.jpeg')
            imwrite(dist_im, img_path, 'Quality', JPEG_QUALITY);
        else
            % Для png/bmp/webp и т.п. пишем без параметра Quality
            imwrite(dist_im, img_path);
        end
    catch
        fprintf('[WARN] Не удалось записать: %s\n', img_path);
        continue;
    end

    % обновляем поля в таблице (логически: это "синтетический under")
    tb.dist_type(i)  = dist_type;
    tb.dist_level(i) = dist_level;

    % blur/over оставляем 0, under уже 1, night уже 0 (как и было)
    tb.blur(i) = 0;
    tb.over(i) = 0;

    processed = processed + 1;

    if mod(processed, 200) == 0
        fprintf('[INFO] Обработано %d / %d\n', processed, numel(idx));
    end
end

%% write updated CSV back
writetable(tb, OUTPUT_CSV);
fprintf('[OK] Готово.\n');
fprintf('  Перезаписано изображений: %d\n', processed);
fprintf('  Пропущено (нет файла): %d\n', skipped_missing);
fprintf('  Пропущено (ошибка чтения): %d\n', skipped_readerr);
fprintf('  CSV обновлён: %s\n', OUTPUT_CSV);
```)[ Модифицированный код добавления искажений ] <матлаб-модификация-2>

В результате для хранения разметки был сформирован единый файл night_labels.csv с дополнительным полем night, которого нет в первом датасете.

\
\
\
\

=== Разделение датасета
После формирования night_labels.csv данные были разделены на обучающую и валидационную выборки. Скрипт, разделяющий датасет, полностью аналогичен скрипту для первой версии, за исключением доработки для работы с полем night.

Скрипт, разделяющий датасет на обучающую и валидационную выборки содержится в листинге @скрипт-разметка-2 и в приложении A.

#листинг(```
import pandas as pd
import numpy as np

LABELS_CSV = "night_labels.csv"
TRAIN_OUT = "night_train.csv"
VAL_OUT = "night_val.csv"

VAL_RATIO = 0.20
SEED = 42

df = pd.read_csv(LABELS_CSV)

# пути под Linux
df["path"] = df["path"].astype(str).str.replace("\\", "/", regex=False)
df["path"] = df["path"].str.replace("//", "/", regex=False)
df["ref"]  = df["ref"].astype(str).str.replace("\\", "/", regex=False)

rng = np.random.RandomState(SEED)

# stratified split по night на уровне ref
ref_night = df.groupby("ref")["night"].max().reset_index()
refs_1 = ref_night.loc[ref_night["night"] == 1, "ref"].values
refs_0 = ref_night.loc[ref_night["night"] == 0, "ref"].values

rng.shuffle(refs_1)
rng.shuffle(refs_0)

n_val_1 = int(round(len(refs_1) * VAL_RATIO))
n_val_0 = int(round(len(refs_0) * VAL_RATIO))

val_refs = set(refs_1[:n_val_1]) | set(refs_0[:n_val_0])
train_refs = set(refs_1[n_val_1:]) | set(refs_0[n_val_0:])

train_df = df[df["ref"].isin(train_refs)].reset_index(drop=True)
val_df   = df[df["ref"].isin(val_refs)].reset_index(drop=True)

train_df.to_csv(TRAIN_OUT, index=False)
val_df.to_csv(VAL_OUT, index=False)

print("Total rows:", len(df))
print("Train rows:", len(train_df), " Val rows:", len(val_df))
print("Night share total:", df["night"].mean())
print("Night share train:", train_df["night"].mean(), " val:", val_df["night"].mean())
print("Example path (train):", train_df.loc[0, "path"])
```)[ Скрипт, разделяющий датасет на обучающую и валидационную выборки ] <скрипт-разметка-2>

В результате были получены файлы night_train.csv и night_val.csv, содержащие как изображения, так и разметку, при этом доля ночных сцен в обеих выборках сохраняется близкой к исходной. Это обеспечивает стабильную проверку качества головы night.

== Третья итерация
Процесс обучения был разделён на три этапа. На первом этапе (5 эпох) общая часть была заморожена, и обучались только головы дефектов качества (blur/under/over); голова night в этот момент не участвовала в оптимизации. На втором этапе (ещё 5 эпох) выполнялась тонкая настройка верхних слоёв бэкбона: слои пакетной нормализации BatchNorm фиксировались, нижняя часть EfficientNet оставалась замороженной, а обучение шло с уменьшенной скоростью обучения. На третьем этапе (5 эпох) обучалась только голова night на отдельном датасете ночных/дневных сцен; при этом вклад остальных голов в функцию потерь отключался весами.

Качество голов blur/under/over оценивалось на обычной валидации val.csv. Итоговые значения не отличаются от значений второй итерации - что не удивительно, потому что изменения в обучение трех изначальных голов не вводились.

Качество головы night оценивалось отдельно на night_val.csv. Получены значения: TP=1414, FP=28, FN=59, TN=2782, что соответствует precision 0.9806, recall 0.9599, F1 0.9702 и accuracy 0.9797. Эти результаты показывают, что модель уверенно отделяет ночные сцены от дневных, а значит может использоваться как надёжный контекстный фильтр для under.

В процессе проверки оказалось, что среди всех ложных срабатываний under на val.csv в 66.86% случаев (234 из 350) ночная голова выдаёт night_pred = 1. В таком случае потенциально устраняется около 234 ложных срабатываний under, и остаётся порядка 116 FP, что уже сопоставимо с уровнем FP для over (106) и существенно ближе к "стабильным" головам (blur/over/night), FP которых равен около 30. Практически это подтверждает, что идея отдельной головы night является работоспособной именно как механизм снижения ложных браков по under.

Результаты проверки точности представлены на рисунке @третье-точность.

#рис(image("../материалы/третье_поколение_accuracy.jpg", width: 60%))[ Точность предсказаний третьей итерации ] <третье-точность>

\
\

Однако график ключевых метрик ведет себя странно после 10-й эпохи. Это объясняется тем, что кривые строятся единым графиком для всех 15 эпох, однако начиная с 11-й эпохи меняется датасет и цель оптимизации: обучение переключается на задачу night, а метки blur/under/over на этом этапе не являются целевыми и фактически отключаются весами. Из-за этого значения метрик по головам качества на третьем этапе теряют смысл (они считаются на данных другого распределения, при иной схеме весов и вообще говоря датасет на night не размечен под over, under и blur), а общая AUC, построенная по всем меткам, также становится некорректной для сравнения с первыми 10 эпохами. Поэтому в третьей итерации интерпретацию обучения следует выполнять не по объединённому графику, а по итоговым метрикам проверки точности на соответствующих валидационных наборах: val.csv для blur/under/over и night_val.csv для night. Графики обучения представлены на рисунке @третье-графики.

#рис(image("../материалы/третье_поколение_training_curves.png", width: 100%))[ Графики ключевых метрик третьей итерации ] <третье-графики> 

Таким образом, полученная версия модели является финальной для текущего этапа работ: она демонстрирует высокое качество по дефектам blur/over и приемлемое качество по under, а добавление головы night даёт практический механизм снижения ложных срабатываний under на ночных сценах в прикладной логике приложения - можно использовать правило: если кадр распознан как ночной, фильтрацию по under не применять. Текущая конфигурация подходит для встраивания в дипломное приложение. Программный код 
третьей итерации представлен в приложении А.
